na.rm=TRUE))%>%filter(Class == 4)
bc.df <- bc.df %>%
mutate(`Bare Nuclei Revised` = ifelse(`Bare Nuclei` == "?" & Class == 2,
round(mean.BareNuclei.2$Mean,2),
ifelse(`Bare Nuclei` == "?" & Class == 4,
round(mean.BareNuclei.4$Mean,2), `Bare Nuclei`)))
bc.df$`Bare Nuclei Revised` <- as.integer(bc.df$`Bare Nuclei Revised`)
bc.df$Class = as.factor(bc.df$Class)
bc.df
bc.df%>%filter(`Bare Nuclei`!= `Bare Nuclei Revised`)
set.seed(17)
num.obs <- dim(bc.df)[1]
num.obs
prop.train <- 0.7
num.train <- round(num.obs*prop.train)
train.indices <- sample(1:num.obs, num.train, replace=FALSE)
train.bc.df <- bc.df[train.indices,]
test.bc.df <- bc.df[-train.indices,]
train.bc.df
test.bc.df
library(knitr)
library(GGally)
train.bc.df.class <- train.bc.df$Class
train.bc.df.X <- select(train.bc.df, -c("Id","Bare Nuclei","Class"))
test.bc.df.class <- test.bc.df$Class
test.bc.df.X <- select(test.bc.df, -c("Id","Bare Nuclei","Class"))
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2)
cor(train.bc.df.X)
cor(train.bc.df.X)
round(apply(train.bc.df.X, 2, sd),3)
train.bc.df.X.std <- scale(train.bc.df.X, center=TRUE, scale=TRUE)
apply(train.bc.df.X.std, 2,sd)
data.pca <- prcomp(train.bc.df.X.std)
summary(data.pca)
screeplot(data.pca, type = "line", main = "Variance Explained by each PC")
screeplot(data.pca, type = "line", main = "Variance Explained by each PC")
round(data.pca$rotation[, 1:2], 3)
PC1 <- data.pca$rotation[, 1]
PC1
PC2 <- data.pca$rotation[, 2]
PC2
PC.scores.1 <- train.bc.df.X.std %*% PC1
PC.scores.2 <- train.bc.df.X.std %*% PC2
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class)
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class)
num.keep <- 2
pca.fit <- prcomp(train.bc.df.X, center=TRUE, scale=TRUE)
pca.fit
train.pca <- as.data.frame(pca.fit$x[, 1:num.keep])
# Create a new dataframe to hold the prcomp() transformed testing data.
test.pca <- as.data.frame(predict(pca.fit, test.bc.df.X)[, 1:num.keep])
# Re-attach the Class to the PCA-trainsformed training and testing dataframes.
train.pca$Class <- train.bc.df.class
test.pca$Class <- test.bc.df.class
library(caret)
library(kernlab)
TrControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 20)
knn.model <- train(Class ~ ., data = train.pca,
method = "knn",
trControl = TrControl,
tuneGrid = expand.grid(k = seq(1, 9, by=2)))
lda.model <- train(Class ~ ., data = , train.pca,
method = "lda",
trControl = TrControl)
qda.model <- train(Class ~ ., data = , train.pca,
method = "qda",
trControl = TrControl)
tree.model <- train(Class ~ ., data = train.pca,
method = "rpart",
trControl = TrControl,
tuneGrid = expand.grid(cp = seq(0.05, 0.5, by=0.05)))
svm.model <- train(Class ~ ., data = train.pca,
method = "svmRadial",
trControl = TrControl,
tuneGrid = expand.grid(C = 2^c(-2:2),
sigma = 2^(-3:3)))
resamp <- resamples(list(KNN = knn.model, LDA = lda.model, QDA = qda.model,
TREE = tree.model, SVM = svm.model))
mmary(resamp)
summary(resamp)
bwplot(resamp)
tree.preds <- predict(tree.model, test.pca)
tree.preds
# Make a confusion matrix for the tree model.
tree.emat <- errormatrix(test.pca$Class, tree.preds,
relative = TRUE)
tree.emat <- errormatrix(test.pca$Class, tree.preds,
relative = TRUE)
library(klaR)
tree.emat <- errormatrix(test.pca$Class, tree.preds,
relative = TRUE)
tree.emat
round(tree.emat, 3)
knn.preds <- predict(knn.model, test.pca)
# Make a confusion matrix for the knn model.
knn.emat <- errormatrix(test.pca$Class, knn.preds,
relative = TRUE)
# Display the confusion matrix for the knn model.
round(knn.emat, 3)
table(bc.df$Class)
bc.df.mean <- bc.df %>%
group_by(Class) %>%
summarize(Mean = mean(as.numeric(`Bare Nuclei`),na.rm=TRUE))
bc.df.mean
bc.df.mean <- bc.df %>% group_by(Class) %>% summarise_all("mean")
bc.df.mean
bc.df.mean <-  select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.df.mean <-  bc.df %>% select(-c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.df.mean <- select(bc.df,-c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
library(tidyverse)
select(bc.df,-c("Id","Bare Nuclei"))
bc.df.mean <- select(bc.df,-c(Id,`Bare Nuclei`)) %>% group_by(Class) %>% summarise_all("mean")
select(bc.df, -c("Id","Bare Nuclei"))
library(tidyverse)
##### DATA CLEANING #####
# Read in data set.
#setwd("C:/Users/17739/Documents/GitHub/BreastCancerDetection/Breast Cancer Git Hub/")
bc.df <- read.table('Data/breast-cancer-wisconsin.data', sep=',')
setwd("C:/Users/17739/Documents/GitHub/BreastCancerDetection/Breast Cancer Git Hub/")
bc.df <- read.table('Data/breast-cancer-wisconsin.data', sep=',')
# Add informative column names.
colnames(bc.df) = c('Id', 'Clump Thickness', 'Cell Size Uniformity', 'Cell Shape Uniformity',
'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',
'Normal Nucleoli', 'Mitosis', 'Class')
#head(bc.df) # View first 6 rows of data.
#dim(bc.df) # Check the dimensions of the data.
#str(bc.df) # Bare Nuclei is only feature stored as chr instead of int.
#table(bc.df$Class) # Report the frequency for each class.
# Write dataframe to a csv file.
#write.csv(bc.df,"breast-cancer-wisconsin.csv", row.names = FALSE)
# Identify missing data.
#bc.df %>% filter(!`Bare Nuclei` %in% c("1","2","3","4","5","6","7","8","9","10"))
# Calculte mean Bare Nuclei by Class.
mean.BareNuclei.2 <- bc.df %>%
group_by(Class) %>%
summarize(Mean = mean(as.numeric(`Bare Nuclei`),
na.rm=TRUE))%>%filter(Class == 2)
mean.BareNuclei.4 <- bc.df %>%
group_by(Class) %>%
summarize(Mean = mean(as.numeric(`Bare Nuclei`),
na.rm=TRUE))%>%filter(Class == 4)
# Impute missing Bare Nuclei data with the mean for each Class mean.
#Bare Nuclei (class 2) with mean Bare Nuclei for class 2
#Bare Nuclei (class 4) with mean Bare Nuclei for class 4
bc.df <- bc.df %>%
mutate(`Bare Nuclei Revised` = ifelse(`Bare Nuclei` == "?" & Class == 2,
round(mean.BareNuclei.2$Mean,2),
ifelse(`Bare Nuclei` == "?" & Class == 4,
round(mean.BareNuclei.4$Mean,2), `Bare Nuclei`))) #%>%
#filter(`Bare Nuclei` == "?")
# Convert Bare Nuclei Revised data to the int data type.
bc.df$`Bare Nuclei Revised` <- as.integer(bc.df$`Bare Nuclei Revised`)
#str(bc.df)
#write revised dataframe to csv
#write.csv(bc.df,"breast-cancer-wisconsin_revised.csv", row.names = FALSE)
# Convert Class to a factor variable.
bc.df$Class = as.factor(bc.df$Class)
bc.df.mean <- select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.df
select(bc.df, -c("Id","Bare Nuclei"))
select(bc.df, -Id)
select(bc.df, -"Id")
select(bc.df, -"Id")
select(bc.df, -c("Id","Bare Nuclei"))
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.df.mean
gather(bc.df.mean, "Feature", "Mean")
help(gather)
pivot_longer(bc.df.mean, names_to = "Feature", values_to = "Mean")
pivot_longer(bc.df.mean, names_to = -"Class", values_to = "Mean")
t(bc.df.mean)
t(as.matrix(bc.df.mean))
t(as.dataframe(bc.df.mean))
t(as.data.frame(bc.df.mean))
t(bc.df.mean)%>%rename("Class 2", "Class 4")
t(select(bc.df.mean,-"Class"))
t(dplyr::select(bc.df.mean,-"Class"))
t(dplyr::select(bc.df.mean,-"Class"))%>%rename("Class 2", "Class 4")
as_tibble(t(dplyr::select(bc.df.mean,-"Class")))
as_tibble(t(dplyr::select(bc.df.mean,-"Class")))%>%rename("Class 2", "Class 4")
as_tibble(t(dplyr::select(bc.df.mean,-"Class")))%>%rename("Class 2" = [,1] , "Class 4" = [,2])
colnames(bc.df.mean) <- c('Benign', 'Malignant')
bc.df.mean
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
#transpose from wide to long
bc.df.mean<-as_tibble(t(dplyr::select(bc.df.mean,-"Class")))
colnames(bc.df.mean) <- c('Benign', 'Malignant')
bc.df.mean
rownames(bc.df.mean)
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.df.mean
colnames(bc.df.mean)
colnames(bc.df.mean)[,-1]
colnames(bc.df.mean)[-1]
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.bf.mean.rownames<-colnames(bc.df.mean)[-1]
#transpose from wide to long
bc.df.mean<-as_tibble(t(dplyr::select(bc.df.mean,-"Class")))
colnames(bc.df.mean) <- c('Benign', 'Malignant')
rownames(bc.df.mean) <- bc.bf.mean.rownames
bc.df.mean
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.df.mean
bc.bf.mean.rownames<-colnames(bc.df.mean)[-1]
bc.bf.mean.rownames
bc.df.mean<-as_tibble(t(dplyr::select(bc.df.mean,-"Class")))
help(as_tibble)
help(as.tibble)
bc.df.mean
colnames(bc.df.mean) <- c('Benign', 'Malignant')
colnames(bc.df.mean)
bc.bf.mean.rownames
rownames(bc.df.mean) <- bc.bf.mean.rownames
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
bc.bf.mean.rownames<-colnames(bc.df.mean)[-1]
bc.df.mean<-as.data.frame(t(dplyr::select(bc.df.mean,-"Class")))
bc.df.mean
colnames(bc.df.mean) <- c('Benign', 'Malignant')
bc.df.mean
knitr::kable(bc.df.mean,align = 'c', caption =  'Mean Summary Statistics', digits = 2)
footnote(knitr::kable(bc.df.mean,align = 'c', caption =  'Mean Summary Statistics', digits = 2), general_title = "Table 1")
install.packages("kableExtra")
library(kableExtra)
footnote(knitr::kable(bc.df.mean,align = 'c', caption =  'Mean Summary Statistics', digits = 2), general_title = "Table 1")
bc.df.sd <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("sd")
bc.df.sd
bc.df.iqr <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("iqr")
bc.df.iqr <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("IQR")
bc.df.iqr
bc.df.sd <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("sd")
#transpose from wide to long
bc.df.sd<-as.data.frame(t(dplyr::select(bc.df.sd,-"Class")))
colnames(bc.df.sd) <- c('Benign', 'Malignant')
knitr::kable(bc.df.sd,align = 'c', caption =  'Table 1: Standard Deviation Summary Statistics', digits = 3)
R
R.version.string
dim(train.bc.df.X.std)
dim(test.bc.df.X.std)
dim(test.bc.df.X)
help("prcomp")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
setwd("C:/Users/17739/Documents/GitHub/BreastCancerDetection/Breast Cancer Git Hub/")
bc.df <- read.table('Data/breast-cancer-wisconsin.data', sep=',')
# Add informative column names.
colnames(bc.df) = c('Id', 'Clump Thickness', 'Cell Size Uniformity', 'Cell Shape Uniformity',
'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',
'Normal Nucleoli', 'Mitosis', 'Class')
mean.BareNuclei.2 <- bc.df %>%
group_by(Class) %>%
summarize(Mean = mean(as.numeric(`Bare Nuclei`),
na.rm=TRUE))%>%filter(Class == 2)
mean.BareNuclei.4 <- bc.df %>%
group_by(Class) %>%
summarize(Mean = mean(as.numeric(`Bare Nuclei`),
na.rm=TRUE))%>%filter(Class == 4)
bc.df <- bc.df %>%
mutate(`Bare Nuclei Revised` = ifelse(`Bare Nuclei` == "?" & Class == 2,
round(mean.BareNuclei.2$Mean,2),
ifelse(`Bare Nuclei` == "?" & Class == 4,
round(mean.BareNuclei.4$Mean,2), `Bare Nuclei`))) #%>%
bc.df$`Bare Nuclei Revised` <- as.integer(bc.df$`Bare Nuclei Revised`)
bc.df$Class = as.factor(bc.df$Class)
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
#transpose from wide to long
bc.df.mean<-as.data.frame(t(dplyr::select(bc.df.mean,-"Class")))
colnames(bc.df.mean) <- c('Benign', 'Malignant')
bc.df.sd <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("sd")
#transpose from wide to long
bc.df.sd<-as.data.frame(t(dplyr::select(bc.df.sd,-"Class")))
colnames(bc.df.sd) <- c('Benign', 'Malignant')
#Formatting Tables
knitr::kable(bc.df.mean,align = 'c', caption =  'Mean Summary Statistics', digits = 2)
knitr::kable(bc.df.sd,align = 'c', caption =  'Standard Deviation Summary Statistics', digits = 3)
library(knitr)
library(GGally)
# Split the training and testing data sets into independent variables and dependent variables.
train.bc.df.class <- train.bc.df$Class
help(median_)
help(median)
knitr::opts_chunk$set(echo = TRUE)
suppressWarnings(library(tidyverse))
setwd("C:/Users/17739/Documents/GitHub/BreastCancerDetection/Breast Cancer Git Hub/")
bc.df <- read.table("Data/breast-cancer-wisconsin.data", sep=",")
setwd("C:/Users/17739/Documents/GitHub/BreastCancerDetection/Breast Cancer Git Hub/")
# Read in the Breast Cancer data set.
bc.df <- read.table("Data/breast-cancer-wisconsin.data", sep=",")
colnames(bc.df) = c("Id", "Clump Thickness", "Cell Size Uniformity",
"Cell Shape Uniformity", "Marginal Adhesion",
"Single Epithelial Cell Size", "Bare Nuclei",
"Bland Chromatin", "Normal Nucleoli", "Mitosis",
"Class")
median.BareNuclei.2 <- bc.df %>%
group_by(Class) %>%
summarize(Median = median(as.numeric(`Bare Nuclei`),
na.rm=TRUE))%>%filter(Class == 2)
median.BareNuclei.2
median.BareNuclei.4 <- bc.df %>%
group_by(Class) %>%
summarize(Median = median(as.numeric(`Bare Nuclei`),
na.rm=TRUE))%>%filter(Class == 4)
median.BareNuclei.4
bc.df <- bc.df %>%
mutate(`Bare Nuclei Revised` = ifelse(`Bare Nuclei` == "?" & Class == 2,
round(median.BareNuclei.2$Median,2),
ifelse(`Bare Nuclei` == "?" & Class == 4,
round(median.BareNuclei.4$Median,2), `Bare Nuclei`)))
bc.df$`Bare Nuclei Revised` <- as.integer(bc.df$`Bare Nuclei Revised`)
bc.df$Class = as.factor(bc.df$Class)
write.csv(bc.df,"breast-cancer-wisconsin_revised.csv", row.names = FALSE)
##### SUMMARY STATISTICS #####
bc.df.mean <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("mean")
#transpose from wide to long
bc.df.mean<-as.data.frame(t(dplyr::select(bc.df.mean,-"Class")))
colnames(bc.df.mean) <- c("Benign", "Malignant")
bc.df.sd <- dplyr::select(bc.df, -c("Id","Bare Nuclei")) %>% group_by(Class) %>% summarise_all("sd")
#transpose from wide to long
bc.df.sd<-as.data.frame(t(dplyr::select(bc.df.sd,-"Class")))
colnames(bc.df.sd) <- c("Benign", "Malignant")
##### DATA SPLITTING #####
# Set a random seed for reproducibility.
set.seed(17)
# Count the number of observations in the data set.
num.obs <- dim(bc.df)[1]
# Set the proportion of observations to be in the training data set.
prop.train <- 0.7
# Set the number of observations to be in the training data set.
num.train <- round(num.obs*prop.train)
# set the indicies of the dataset to be in the training data set.
train.indices <- sample(1:num.obs, num.train, replace=FALSE)
# Subset the data into training and testing data sets.
train.bc.df <- bc.df[train.indices,]
test.bc.df <- bc.df[-train.indices,]
# Save the vector of the indicies of the independent variable columns.
X_col_indicies <- c(2, 3, 4, 5, 6, 8, 9, 10, 12)
knitr::kable(bc.df.mean, align = "c",
caption =  "Mean Summary Statistics", digits = 2)
knitr::kable(bc.df.sd, align = "c",
caption =  "Standard Deviation Summary Statistics", digits = 3)
pairs(bc.df[,-c(1,10)],
col = c(4, 2)[as.numeric(bc.df$Class)],   # Change color by group
pch = c(8, 1)[as.numeric(bc.df$Class)],   # Change points by group
main = "Pair Plot of Benign and Malignant for All Variables", oma=c(2,2,2,16))
head(bc.df)
pairs(bc.df[,-c(1,7,11)],
col = c(4, 2)[as.numeric(bc.df$Class)],   # Change color by group
pch = c(8, 1)[as.numeric(bc.df$Class)],   # Change points by group
main = "Pair Plot of Benign and Malignant for All Variables", oma=c(2,2,2,16))
par(xpd = TRUE)
legend("bottomright", col=c(4, 2),pch = c(8, 1),
legend = c("Benign", "Malignant"))
par(mfrow=c(3,3))
for(col_index in X_col_indicies){
boxplot(bc.df[,col_index] ~ Class, data = bc.df, col=c(4,2),
xlab="", ylab = "", main=names(bc.df)[col_index])
par(mfrow=c(3,3))
for(col_index in X_col_indicies){
boxplot(bc.df[,col_index] ~ Class, data = bc.df, col=c(4,2),
xlab="", ylab = "", main=names(bc.df)[col_index])
}
}
suppressWarnings(library(ICSNP))
# Seperate the 2 classes into dataframes for mean vector comparison.
X = as.matrix(bc.df[bc.df$Class == "2", X_col_indicies]) # class 2
Y = as.matrix(bc.df[bc.df$Class == "4", X_col_indicies]) # class 4
X
head(X)
chisquare.plot <- function(x, mark, title) {
# x = A nxp data matrix, mark: number of extreme points to mark,
# p = number of variables, n = sample size
p <- ncol(x)
n <- nrow(x)
# xbar and s
xbar <- colMeans(x)
s <- cov(x)
# Mahalanobis dist, sorted
x.cen <- scale(x, center = T, scale = F)
d2 <- diag(x.cen %*% solve(s) %*% t(x.cen))
sortd <- sort(d2)
# chi-sq quantiles
qchi <- qchisq((1:n - 0.5)/n, df = p)
# plot, mark points with heighest distance
plot(qchi, sortd, pch = 19, xlab = "Chi-square quantiles",
ylab = "Mahalanobis squared distances",
main = title)
points(qchi[(n - mark + 1):n], sortd[(n - mark + 1):n], cex = 3, col = "#990000")
}
par(mfrow=c(1,2))
chisquare.plot(X,2, "Chi-square Q-Q Plot of Benign")
chisquare.plot(Y,2, "Chi-square Q-Q Plot of Malignant")
HotellingsT2(X,Y)
alpha = 0.05 # old significance level
p = 2 # number of intervals/variables
# mean vectors
xbar = colMeans(X)
ybar = colMeans(Y)
difference = xbar - ybar
# covariances of each group
S.x = cov(X)
S.y = cov(Y)
# bc.df statistics summary
stats = round(cbind(xbar, ybar, sqrt(diag(S.x)), sqrt(diag(S.y))),3)
colnames(stats) = c("Benign Mean", "Malignant Mean", "Benign Sd", "Malignant Sd")
stats
# sample sizes
m = nrow(X)
n = nrow(Y)
# pooled covariance matrix
S.pool = ((m-1)*S.x + (n-1)*S.y) / (m + n - 2)
# critical value
crit = qt(alpha/(2*p), df = m+n-2, lower.tail = F)
half.width = crit*sqrt(diag(S.pool)*(1/m + 1/n))
lower = difference - half.width
upper = difference + half.width
int.bonf = cbind(difference, lower, upper)
int.bonf
int.bonf = cbind(lower, upper)
int.bonf
int.bonf = round(cbind(lower, upper),4)
int.bonf
int.bonf = round(cbind(lower, upper),3)
int.bonf
suppressWarnings(library(knitr))
suppressWarnings(library(GGally))
train.bc.df.class <- train.bc.df$Class
train.bc.df.X <- select(train.bc.df, -c("Id","Bare Nuclei","Class"))
test.bc.df.class <- test.bc.df$Class
test.bc.df.X <- select(test.bc.df, -c("Id","Bare Nuclei","Class"))
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2)
cor(train.bc.df.X)
train.bc.df.X.std <- scale(train.bc.df.X, center=TRUE, scale=TRUE)
# Perform PCA.
data.pca <- prcomp(train.bc.df.X.std)
# Extract the importance of each component.
# standard deviation
st.dev <- data.pca$sdev
# variance
var <- st.dev^2
# total variance
TV <- sum(var)
#proportion of variance explained
prop <- var/TV
#cumulative proportion of variance explained
pve <- cumsum(var)/TV
#combine in a table
tab <- rbind(st.dev, prop, pve)
rownames(tab) <- c("Standard deviation", "Proportion of variance",
"Cumulative proportion")
colnames(tab) <- paste0("PC",1:9)
knitr::kable(tab, align = "c",
caption = "PCA Importance of Components", digits = 3)
screeplot(data.pca, type = "line", main = "Variance Explained by each PC")
knitr::kable(data.pca$rotation[, 1:2], align = "c",
caption = "Loadings of the First Two PCs", digits = 3)
PC1 <- data.pca$rotation[, 1]
PC2 <- data.pca$rotation[, 2]
PC.scores.1 <- train.bc.df.X.std %*% PC1
PC.scores.2 <- train.bc.df.X.std %*% PC2
# Plot PC score 1 vs PC score 2 colored by class.
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class)
help(prcomp)
train.df = train.bc.df.X
test.df = test.bc.df.X
# Re-attach the Class to the independent variables of the training and testing dataframes.
train.df$Class <- train.bc.df.class
test.df$Class <- test.bc.df.class
# This is essential to get rpart to run on this data.
colnames(train.df) <- make.names(colnames(train.df))
colnames(test.df) <- make.names(colnames(train.df))
# Set the number of principal components to keep.
num.keep <- 2
# Re-fit prcomp() so it knows to center and scale input data.
pca.fit <- prcomp(train.bc.df.X, center=TRUE, scale=TRUE)
summary(pca.fit)
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class, main = "PC1 Score vs PC2 Scores")
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class, main = "PC1 Scores vs PC2 Scores", xlab = "PC1 Scores", ylab = "PC2 Scores")
legend(-2, 4, legend=c("Class 2: Benign", "Class 4: Malignant"),
col=c("black", "red"), lty=1:2, cex=0.8)
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class, main = "PC1 Scores vs PC2 Scores", xlab = "PC1 Scores", ylab = "PC2 Scores")
legend(-2, 4, legend=c("Class 2: Benign", "Class 4: Malignant"),
col=c("black", "red"), lty=1:2, cex=0.8)
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class, main = "PC1 Scores vs PC2 Scores", xlab = "PC1 Scores", ylab = "PC2 Scores")
legend("bottomleft", legend=c("Class 2: Benign", "Class 4: Malignant"),
col=c("black", "red"), lty=1:2, cex=0.8)
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class, main = "PC1 Scores vs PC2 Scores", xlab = "PC1 Scores", ylab = "PC2 Scores")
legend("bottomleft", legend=c("Class 2: Benign", "Class 4: Malignant"),
col=c("black", "red"), cex=0.8)
plot(PC.scores.1,PC.scores.2, pch=19, col=train.bc.df.class, main = "PC1 Scores vs PC2 Scores", xlab = "PC1 Scores", ylab = "PC2 Scores")
legend("bottomleft", legend=c("Class 2: Benign", "Class 4: Malignant"),
col=c("black", "red"), pch=19, cex=0.8)
help(ggcor)
help(ggcorr)
knitr::opts_chunk$set(echo = TRUE)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2, layout.exp = 10)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2, layout.exp = 10)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2, layout.exp = 100)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2, layout.exp = 5)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2, layout.exp = 150)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2, layout.exp = 15)
ggcorr(train.bc.df.X, label=TRUE, label_size=3, label_round=2, layout.exp = 7)
ggcorr(train.bc.df.X, label=TRUE, label_size=5, label_round=2, layout.exp = 7)
ggcorr(train.bc.df.X, label=TRUE, label_size=4, label_round=2, layout.exp = 7)
ggcorr(train.bc.df.X, label=TRUE, label_size=4, label_round=2, layout.exp = 5)
ggcorr(train.bc.df.X, label=TRUE, label_size=4, label_round=2, layout.exp = 5, hjust = "left")
ggcorr(train.bc.df.X, label=TRUE, label_size=4, label_round=2, layout.exp = 5, hjust = "right")
cor(train.bc.df.X)
ggcorr(train.bc.df.X, label=TRUE, label_size=4, label_round=2, layout.exp = 5, hjust = "right")
ggcorr(train.bc.df.X, label=TRUE, label_size=4, label_round=2, layout.exp = 5, hjust = "right")
cor(train.bc.df.X)
